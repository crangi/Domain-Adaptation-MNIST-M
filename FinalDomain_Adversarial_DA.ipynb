{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epVkYGY9dakj"
   },
   "source": [
    "# Beyond Black and White: Adapting models to visual domain shift\n",
    "### Authors: Chakradhar Rangi\n",
    "\n",
    "The goal of this project is to demonstrate the need for and application of Domain Adaptation (DA) using the problem of handwritten digit classification. DA techniques allow neural networks trained on a **source domain** to generalize to an unseen **target domain** without access to target labels. We focus on **covariate shift**, where the input distribution changes (grayscale vs. colored/textured) while the classification task remains the same.\n",
    "\n",
    "In this final notebook, we tackle the limitations of our previous statistical approaches.\n",
    "\n",
    "## Task 3: Training with Adversarial Domain Adaptation (DANN)\n",
    "\n",
    "In our previous experiments, we found that distance-based methods (like MMD) improved target accuracy to ~79% but plateaued. While these methods align statistical moments (means/variances), they do not actively hunt for the specific features that distinguish the two domains. To bridge this final gap, we implement **Domain-Adversarial Training of Neural Networks (DANN)**.\n",
    "\n",
    "Introduced by [Ganin et al. (2016)](https://arxiv.org/abs/1505.07818), this approach draws inspiration from Generative Adversarial Networks (GANs). It introduces a \"minimax\" game between the feature extractor and a new component: the **Domain Discriminator**. The goal is to train the feature extractor to produce **domain-invariant features**—representations that are discriminative enough to classify digits correctly, yet so generic that the discriminator cannot tell if they came from the Source or Target.\n",
    "\n",
    "### The Architecture: Three Components\n",
    "We modify our standard CNN to include three distinct functional blocks:\n",
    "1.  **Feature Extractor ($G_f$):** Maps inputs to the latent representation $Z$.\n",
    "2.  **Label Predictor ($G_y$):** The standard classifier that predicts digit labels ($0-9$) from $z$.\n",
    "3.  **Domain Classifier ($G_d$):** A binary classifier that tries to predict the domain label ($d \\in \\{0, 1\\}$) from $Z$.\n",
    "\n",
    "\n",
    "\n",
    "### The Gradient Reversal Layer (GRL)\n",
    "The \"adversarial\" magic happens via a **Gradient Reversal Layer (GRL)** placed between the Feature Extractor and the Domain Classifier.\n",
    "* **Forward Pass:** The GRL acts as an identity function (it does nothing).\n",
    "* **Backward Pass:** The GRL **multiplies the gradient by $-\\alpha$**.\n",
    "\n",
    "This reversal signals the Feature Extractor to minimize the label prediction loss while simultaneously **maximizing the domain classification loss**. In other words, the Feature Extractor learns to \"confuse\" the Domain Classifier.\n",
    "\n",
    "### Mathematical Formulation\n",
    "Let $X_s, Y_s$ be source examples with labels, and $X_t$ be target examples. We assign binary domain labels $d_s=0$ for source data and $d_t=1$ for target data. We optimize two loss functions simultaneously:\n",
    "\n",
    "**1. Task Loss (Supervised):**\n",
    "Standard Cross-Entropy loss on the source domain:\n",
    "$$\\mathcal{L}_y(\\theta_f, \\theta_y) = \\mathcal{L}_{\\text{CE}}(G_y(G_f(X_s)), Y_s)$$\n",
    "\n",
    "**2. Domain Loss (Adversarial):**\n",
    "Binary Cross-Entropy loss on *both* domains:\n",
    "$$\\mathcal{L}_d(\\theta_f, \\theta_d) = \\mathcal{L}_{\\text{BCE}}(G_d(G_f(X_s)), d_s) + \\mathcal{L}_{\\text{BCE}}(G_d(G_f(X_t)), d_t)$$\n",
    "\n",
    "**The Total Objective:**\n",
    "$$E(\\theta_f, \\theta_y, \\theta_d) = \\mathcal{L}_y(\\theta_f, \\theta_y) - \\alpha \\cdot \\mathcal{L}_d(\\theta_f, \\theta_d)$$\n",
    "\n",
    "*(Note: In our PyTorch implementation, we sum the losses and rely on the GRL to flip the sign of the domain gradient during backpropagation.)*\n",
    "\n",
    "During training, we search for parameters such that $\\theta_d$ minimizes the domain classification error, while $\\theta_f$ minimizes the task error but **maximizes** the domain error. The hyperparameter $\\alpha$ controls the trade-off, typically annealing from 0 to 1 to allow the feature extractor to stabilize before the adversary becomes too difficult to fool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22412,
     "status": "ok",
     "timestamp": 1764665633286,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "APX8SEeed8qO",
    "outputId": "551abbf0-8231-4dae-d987-5b18400172af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA A100-SXM4-40GB\n",
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# Importing the required libraries\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Function\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, ImageFolder\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check for GPU availability\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if \"A100\" in gpu_name:\n",
    "      # ENABLE TF32\n",
    "      torch.backends.cuda.matmul.allow_tf32 = True\n",
    "      torch.backends.cudnn.allow_tf32 = True\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU available, using CPU.\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Sets the seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "    # Apple MPS (Mac)\n",
    "    if torch.backends.mps.is_available():\n",
    "        torch.mps.manual_seed(seed)\n",
    "\n",
    "    print(f\"Seed set to {seed}\")\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Setting global variables\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "# mount google drive to save and load files\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0aoWAJFjUlB"
   },
   "source": [
    "### 3.1 Let us download and normalize our datasets for the final time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2918,
     "status": "ok",
     "timestamp": 1764665638495,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "68Wc3N4Gjcgj",
    "outputId": "c3c5f6ec-5b8b-4ac1-941b-1e8dd32896a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 43.5MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 1.08MB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 9.16MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Mean and std of MNIST dataset\n",
    "mnist_mean = (0.1307,) * 3  # Replicated for 3 channels\n",
    "mnist_std = (0.3081,) * 3  # Replicated for 3 channels\n",
    "\n",
    "# Define transform\n",
    "mnist_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mnist_mean, mnist_std)\n",
    "])\n",
    "\n",
    "# Downloading the MNIST dataset from torchvision\n",
    "mnist_train = MNIST(root='./data', train=True, download=True, transform=mnist_transform)\n",
    "mnist_test = MNIST(root='./data', train=False, download=True, transform=mnist_transform)\n",
    "\n",
    "# Split into Train (90%) and Validation (10%)\n",
    "# Validation is used for model checkpointing/tuning.\n",
    "train_size = int(0.9 * len(mnist_train))\n",
    "val_size = len(mnist_train) - train_size\n",
    "train_subset, val_subset = random_split(mnist_train, [train_size, val_size])\n",
    "\n",
    "# Define DataLoaders\n",
    "# pin_memory=True speeds up the transfer of data to the GPU.\n",
    "\n",
    "source_train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "source_val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "source_test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7797,
     "status": "ok",
     "timestamp": 1764665647858,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "DwXu2VRhjffb",
    "outputId": "bd26bb40-c8a0-4e9d-ec6e-d088dd9afe72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MNIST-M.zip...\n",
      "Download complete.\n",
      "Extracting/Verifying data...\n",
      "\n",
      "Scanning ./data for 'training' folder...\n",
      "FOUND training folder at: ./data/MNIST-M/training\n",
      "Paths for training and testing:\n",
      "Train Root: ./data/MNIST-M/training\n",
      "Test Root:  ./data/MNIST-M/testing\n"
     ]
    }
   ],
   "source": [
    "# Downloading MNIST-M dataset from GitHub\n",
    "# Setup paths\n",
    "data_root = './data'\n",
    "zip_name = 'MNIST-M.zip'\n",
    "zip_path = os.path.join(data_root, zip_name)\n",
    "url = \"https://github.com/mashaan14/MNIST-M/raw/main/MNIST-M.zip\"\n",
    "\n",
    "# Ensure Data Folder Exists\n",
    "if not os.path.exists(data_root):\n",
    "    os.makedirs(data_root)\n",
    "    print(f\"Created directory: {data_root}\")\n",
    "\n",
    "# Check if ZIP exists, if not, Download it\n",
    "if not os.path.exists(zip_path):\n",
    "    print(f\"Downloading {zip_name}...\")\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, zip_path)\n",
    "        print(\"Download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Download failed: {e}\")\n",
    "else:\n",
    "    print(f\"Found {zip_name}.\")\n",
    "\n",
    "# Force Extraction (to ensure folders exist)\n",
    "print(\"Extracting/Verifying data...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(data_root)\n",
    "\n",
    "# search for the 'training' folder\n",
    "found_train_path = None\n",
    "print(f\"\\nScanning {data_root} for 'training' folder...\")\n",
    "\n",
    "for root, dirs, files in os.walk(data_root):\n",
    "    if 'training' in dirs:\n",
    "        found_train_path = os.path.join(root, 'training')\n",
    "        print(f\"FOUND training folder at: {found_train_path}\")\n",
    "        break\n",
    "\n",
    "if found_train_path:\n",
    "    # Set the correct paths dynamically\n",
    "    train_root = found_train_path\n",
    "    test_root = found_train_path.replace('training', 'testing')\n",
    "\n",
    "    print(f\"Paths for training and testing:\")\n",
    "    print(f\"Train Root: {train_root}\")\n",
    "    print(f\"Test Root:  {test_root}\")\n",
    "else:\n",
    "    print(\"ERROR: Could not find a 'training' folder after extraction.\")\n",
    "    print(\"Printing current ./data structure:\")\n",
    "    for root, dirs, files in os.walk(data_root):\n",
    "        level = root.replace(data_root, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print(f\"{indent}{os.path.basename(root)}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1764665647986,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "bcfFXIjIjjGL"
   },
   "outputs": [],
   "source": [
    "mnistm_mean = (0.4582, 0.4623, 0.4085)\n",
    "mnistm_std = (0.2386, 0.2239, 0.2444)\n",
    "\n",
    "mnistm_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),        # Resize to 28x28 (Original MNIST size)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mnistm_mean, mnistm_std)\n",
    "])\n",
    "\n",
    "# Load MNIST-M train dataset\n",
    "mnistm_train = ImageFolder(root=train_root, transform=mnistm_transform)\n",
    "mnistm_test = ImageFolder(root=test_root, transform=mnistm_transform)\n",
    "\n",
    "# Define DataLoaders\n",
    "target_train_loader = DataLoader(mnistm_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "target_test_loader = DataLoader(mnistm_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibYKO7y1js0l"
   },
   "source": [
    "### 3.2 Let us redefine our model to include adversarial component.\n",
    "\n",
    "We also have to modify the gradient computation through GRL as discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1764665650989,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "DZLOXIOJdMJn"
   },
   "outputs": [],
   "source": [
    "# Gradient Reversal Layer\n",
    "class GradientReversalFn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Reverse the gradient and scale by alpha\n",
    "        output = grad_output.neg() * ctx.alpha\n",
    "        return output, None\n",
    "\n",
    "class GradientReversalLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        return GradientReversalFn.apply(x, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1764665652572,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "c4wvKZWPd54i"
   },
   "outputs": [],
   "source": [
    "class DANN_CNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # Shared Feature Extractor (Same as Task 1/2)\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "        )\n",
    "\n",
    "        self.flatten_dim = 128 * 7 * 7\n",
    "\n",
    "        # Label Predictor\n",
    "        self.class_classifier = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        # Domain Classifier\n",
    "        # This branch tries to predict Source (0) vs Target (1)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(self.flatten_dim, 100),\n",
    "            nn.BatchNorm1d(100), # BatchNorm helps adversarial stability\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(100, 1)    # Binary classification (Src vs Tgt)\n",
    "        )\n",
    "\n",
    "        self.grl = GradientReversalLayer()\n",
    "\n",
    "    def forward(self, x, alpha=1.0):\n",
    "        # Extract Features\n",
    "        features = self.features(x)\n",
    "        features = features.view(features.size(0), -1)\n",
    "\n",
    "        # Path A: Class Prediction (Standard)\n",
    "        class_logits = self.class_classifier(features)\n",
    "\n",
    "        # Path B: Domain Prediction (Adversarial)\n",
    "        # Apply GRL first! This reverses gradients during backprop.\n",
    "        features_reversed = self.grl(features, alpha)\n",
    "        domain_logits = self.domain_classifier(features_reversed)\n",
    "\n",
    "        return class_logits, domain_logits\n",
    "\n",
    "def evaluate_accuracy(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, _ = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTP23wISFDFS"
   },
   "source": [
    "### 3.3 Training our model: Experiment 1\n",
    "\n",
    "Now that we have defined the GRL, let us proceed with DA. Let us first pretrain the model with the best CNN model from task 1. We have to be a little careful this time initializing our model as it comes with an additional doman classifier component. We just initialize the digit classifer part of the network with the pretrained model from task 1 and randomly initialize the domain classifer. Additionally, we do not reduce the learning rate in this case as we like the model to also quickly learn domain invariant features.\n",
    "\n",
    "As for the initialization and training, we shall start with pre-training our model with a batch size of 256 and also fixed $\\alpha= 0.25$ unlike annealing as discussed in the beginning.\n",
    "\n",
    "Here are the summary of parameters for this initial experiment:\n",
    "1. BATCH SIZE = 256 (initial model) & 256 (training)\n",
    "2. LEARNING RATE = 1e-3\n",
    "3. OPTIMIZER = ADAMW\n",
    "4. LOSS FN = CrossEntropy (class) + BCEWithLogitsLoss (domain)\n",
    "5. ALPHA = 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217337,
     "status": "ok",
     "timestamp": 1764665892448,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "TM8UezYsKdcR",
    "outputId": "6d0cf113-6a16-4226-c7aa-9b2d441ab05f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n",
      " Starting DANN Training (Adversarial)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.001 | Dom Loss: 0.170 | Dom Acc: 98.6%: 100%|██████████| 211/211 [00:09<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0189\n",
      "   Avg Domain Loss: 0.3697\n",
      "   Source Acc: 99.46%\n",
      "   Target Acc: 41.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.008 | Dom Loss: 0.110 | Dom Acc: 99.4%: 100%|██████████| 211/211 [00:08<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0228\n",
      "   Avg Domain Loss: 0.1327\n",
      "   Source Acc: 99.29%\n",
      "   Target Acc: 56.28%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.019 | Dom Loss: 0.080 | Dom Acc: 99.2%: 100%|██████████| 211/211 [00:08<00:00, 25.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0192\n",
      "   Avg Domain Loss: 0.0804\n",
      "   Source Acc: 99.25%\n",
      "   Target Acc: 57.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.012 | Dom Loss: 0.058 | Dom Acc: 99.4%: 100%|██████████| 211/211 [00:08<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0185\n",
      "   Avg Domain Loss: 0.0539\n",
      "   Source Acc: 99.17%\n",
      "   Target Acc: 61.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.010 | Dom Loss: 0.071 | Dom Acc: 99.0%: 100%|██████████| 211/211 [00:08<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0199\n",
      "   Avg Domain Loss: 0.0639\n",
      "   Source Acc: 99.03%\n",
      "   Target Acc: 63.72%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.046 | Dom Loss: 0.244 | Dom Acc: 89.8%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0278\n",
      "   Avg Domain Loss: 0.1100\n",
      "   Source Acc: 97.03%\n",
      "   Target Acc: 48.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.042 | Dom Loss: 0.095 | Dom Acc: 97.3%: 100%|██████████| 211/211 [00:08<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0423\n",
      "   Avg Domain Loss: 0.1750\n",
      "   Source Acc: 98.76%\n",
      "   Target Acc: 44.34%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.023 | Dom Loss: 0.043 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0322\n",
      "   Avg Domain Loss: 0.1218\n",
      "   Source Acc: 99.17%\n",
      "   Target Acc: 55.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.015 | Dom Loss: 0.076 | Dom Acc: 98.2%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0188\n",
      "   Avg Domain Loss: 0.0676\n",
      "   Source Acc: 99.28%\n",
      "   Target Acc: 59.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.024 | Dom Loss: 0.111 | Dom Acc: 97.9%: 100%|██████████| 211/211 [00:07<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0279\n",
      "   Avg Domain Loss: 0.1104\n",
      "   Source Acc: 98.82%\n",
      "   Target Acc: 63.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.026 | Dom Loss: 0.171 | Dom Acc: 93.6%: 100%|██████████| 211/211 [00:07<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0364\n",
      "   Avg Domain Loss: 0.1831\n",
      "   Source Acc: 99.25%\n",
      "   Target Acc: 74.47%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.027 | Dom Loss: 0.431 | Dom Acc: 82.0%: 100%|██████████| 211/211 [00:08<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0456\n",
      "   Avg Domain Loss: 0.2955\n",
      "   Source Acc: 98.93%\n",
      "   Target Acc: 76.62%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.027 | Dom Loss: 0.510 | Dom Acc: 73.8%: 100%|██████████| 211/211 [00:08<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0501\n",
      "   Avg Domain Loss: 0.3937\n",
      "   Source Acc: 99.00%\n",
      "   Target Acc: 83.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.103 | Dom Loss: 0.498 | Dom Acc: 78.9%: 100%|██████████| 211/211 [00:08<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0590\n",
      "   Avg Domain Loss: 0.5012\n",
      "   Source Acc: 98.95%\n",
      "   Target Acc: 78.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.048 | Dom Loss: 0.585 | Dom Acc: 70.3%: 100%|██████████| 211/211 [00:08<00:00, 25.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0526\n",
      "   Avg Domain Loss: 0.5035\n",
      "   Source Acc: 98.89%\n",
      "   Target Acc: 84.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.035 | Dom Loss: 0.409 | Dom Acc: 84.2%: 100%|██████████| 211/211 [00:08<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0495\n",
      "   Avg Domain Loss: 0.5122\n",
      "   Source Acc: 99.21%\n",
      "   Target Acc: 86.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.054 | Dom Loss: 0.608 | Dom Acc: 72.3%: 100%|██████████| 211/211 [00:08<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0436\n",
      "   Avg Domain Loss: 0.5075\n",
      "   Source Acc: 98.59%\n",
      "   Target Acc: 84.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.032 | Dom Loss: 0.805 | Dom Acc: 55.9%: 100%|██████████| 211/211 [00:08<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0452\n",
      "   Avg Domain Loss: 0.5425\n",
      "   Source Acc: 99.01%\n",
      "   Target Acc: 85.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.021 | Dom Loss: 0.489 | Dom Acc: 74.8%: 100%|██████████| 211/211 [00:08<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0445\n",
      "   Avg Domain Loss: 0.5450\n",
      "   Source Acc: 99.21%\n",
      "   Target Acc: 89.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.060 | Dom Loss: 0.546 | Dom Acc: 72.7%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0421\n",
      "   Avg Domain Loss: 0.5663\n",
      "   Source Acc: 99.12%\n",
      "   Target Acc: 88.20%\n",
      "\n",
      "Best model weights saved to /content/drive/MyDrive/DA_Project/models/best_DANN_EXP1256_20_0.001_0.25.pth\n",
      "\n",
      "Adaptation Complete in 214.5s Best Target Accuracy: 89.13%\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "# Target location of our saved model\n",
    "models_path = \"/content/drive/MyDrive/DA_Project/models/\"\n",
    "best_model_file_path = os.path.join(models_path, f\"best_cnn_mnist_{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}.pth\")\n",
    "\n",
    "# Initialize DANN\n",
    "dann_model = DANN_CNN(num_classes=10).to(device)\n",
    "if os.path.exists(best_model_file_path):\n",
    "    print(f\"Loading pre-trained source weights from {best_model_file_path}...\")\n",
    "    pretrained_state_dict = torch.load(best_model_file_path, map_location=device)\n",
    "\n",
    "    # Create a new state_dict for the DANN model, which we'll populate\n",
    "    dann_state_dict = dann_model.state_dict()\n",
    "\n",
    "    # Copy weights for the feature extractor\n",
    "    for k, v in pretrained_state_dict.items():\n",
    "        if k.startswith('features.'):\n",
    "            dann_state_dict[k] = v\n",
    "        elif k.startswith('classifier.'):\n",
    "            # Rename 'classifier' keys to 'class_classifier' to match DANN_CNN\n",
    "            new_key = k.replace('classifier.', 'class_classifier.')\n",
    "            dann_state_dict[new_key] = v\n",
    "\n",
    "    # Load the modified state_dict. strict=False is used because 'domain_classifier'\n",
    "    # keys will be missing from the loaded pretrained_state_dict, which is intended.\n",
    "    dann_model.load_state_dict(dann_state_dict, strict=False)\n",
    "    print(\"Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\\n\")\n",
    "else:\n",
    "    print(\"Pre-trained weights not found. Starting DANN_CNN from scratch.\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_domain = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(dann_model.parameters(), lr=LEARNING_RATE)\n",
    "best_target_acc = 0.0\n",
    "\n",
    "print(f\" Starting DANN Training (Adversarial)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    dann_model.train()\n",
    "\n",
    "    # Stats tracking\n",
    "    total_loss, total_domain_loss, total_class_loss = 0.0, 0.0, 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # fixed alpha\n",
    "    alpha = 0.25\n",
    "\n",
    "    # Zip loaders\n",
    "    min_len = min(len(source_train_loader), len(target_train_loader))\n",
    "    loader_zip = tqdm(zip(source_train_loader, target_train_loader), total=min_len)\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, _) in loader_zip:\n",
    "        # Setup\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs = target_imgs.to(device)\n",
    "        batch_size = source_imgs.size(0)    # we do this once again to make sure of the batch size\n",
    "\n",
    "        # Handle uneven batches\n",
    "        if source_imgs.size(0) != target_imgs.size(0): continue\n",
    "\n",
    "        # Create Domain Labels: Source = 0, Target = 1\n",
    "        domain_label_s = torch.zeros(batch_size,1).float().to(device)\n",
    "        domain_label_t = torch.ones(batch_size,1).float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass (Source Data)\n",
    "        # We calculate BOTH Class Loss and Domain Loss\n",
    "        class_logits_s, domain_logits_s = dann_model(source_imgs, alpha)\n",
    "        loss_class = criterion(class_logits_s, source_labels)\n",
    "        loss_domain_s = criterion_domain(domain_logits_s, domain_label_s)\n",
    "\n",
    "        # Forward Pass (Target Data)\n",
    "        # We ONLY calculate Domain Loss (we don't have class labels)\n",
    "        _, domain_logits_t = dann_model(target_imgs, alpha)\n",
    "        loss_domain_t = criterion_domain(domain_logits_t, domain_label_t)\n",
    "\n",
    "        # Backward Pass (Optimization)\n",
    "        # The GRL inside the model handles the sign flipping automatically!\n",
    "        loss_domain = (loss_domain_s + loss_domain_t)/2\n",
    "        loss = loss_class + loss_domain\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        total_loss += loss.item()\n",
    "        total_class_loss += loss_class.item()\n",
    "        total_domain_loss += loss_domain.item()\n",
    "        total_samples += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob_s = torch.sigmoid(domain_logits_s).mean().item()\n",
    "            prob_t = torch.sigmoid(domain_logits_t).mean().item()\n",
    "\n",
    "            # Accuracy checks\n",
    "\n",
    "            # Source is correctly classified if logit < 0 (Prob < 0.5)\n",
    "            acc_s = (domain_logits_s < 0).float().mean().item() * 100\n",
    "            # Target is correctly classified if logit > 0 (Prob > 0.5)\n",
    "            acc_t = (domain_logits_t > 0).float().mean().item() * 100\n",
    "\n",
    "            domain_acc = (acc_s + acc_t) / 2\n",
    "\n",
    "        # Logging\n",
    "        loader_zip.set_description(\n",
    "            f\"Ep {epoch+1} | Cls: {loss_class.item():.3f} | \"\n",
    "            f\"Dom Loss: {loss_domain.item():.3f} | \"\n",
    "            f\"Dom Acc: {domain_acc:.1f}%\"\n",
    "        )\n",
    "\n",
    "        # Average classification loss and domain loss\n",
    "        avg_class_loss = total_class_loss / total_samples\n",
    "        avg_domain_loss = total_domain_loss / total_samples\n",
    "\n",
    "    # Validation\n",
    "    # Evaluate on Target (Did adaptation work?)\n",
    "    target_acc = evaluate_accuracy(dann_model, target_test_loader, device)\n",
    "\n",
    "    # Evaluate on Source (Did we preserve original knowledge?)\n",
    "    source_acc = evaluate_accuracy(dann_model, source_test_loader, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Stats:\")\n",
    "    print(f\"   Avg CL Loss: {avg_class_loss:.4f}\")\n",
    "    print(f\"   Avg Domain Loss: {avg_domain_loss:.4f}\")\n",
    "    print(f\"   Source Acc: {source_acc:.2f}%\")\n",
    "    print(f\"   Target Acc: {target_acc:.2f}%\\n\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if target_acc > best_target_acc:\n",
    "        best_target_acc = target_acc\n",
    "        best_model_wts = copy.deepcopy(dann_model.state_dict())\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Save the best model weights\n",
    "target_file_path_best = os.path.join(models_path, f\"best_DANN_EXP1{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}_{alpha}.pth\")\n",
    "torch.save(best_model_wts, target_file_path_best)\n",
    "print(f\"Best model weights saved to {target_file_path_best}\")\n",
    "print(f\"\\nAdaptation Complete in {total_time:.1f}s Best Target Accuracy: {best_target_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Un1Zjkc9ya1r"
   },
   "source": [
    "### Takeaway experiment 1: Optimized DANN\n",
    "\n",
    "In this primary adversarial experiment, we initializing the DANN with a source model pre-trained on a batch size of 256 and performing adversarial adaptation using the same batch size of 256. We utilized an aggressive learning rate of $1e^{-3}$ and a **fixed weak adversarial strength of $\\alpha = 0.25$**.\n",
    "\n",
    "This configuration yielded the project's state-of-the-art performance, achieving a target accuracy of **89.13%**. This significantly outperforms:\n",
    "- The Distance-based (MMD) baseline: **79.36%**\n",
    "- Original DANN paper (2015): **76.1% ± 1.8%**\n",
    "\n",
    "The superior performance of this training supports two critical findings. First, pretraining the model with a good baseline helps significantly compared to training from scratch (as done in the original DANN paper), as it provides the feature extractor with robust semantic representations before the adversary begins. Second, the fixed weak alpha ($\\alpha=0.25$) allowed for \"Sufficient Alignment\"—aligning semantic digit features while ignoring enough background noise to generalize—without the instability or feature distortion often caused by the stronger adversarial pressure of standard annealing schedules.\n",
    "\n",
    "### Robustness Check: Re-running Experiment 1 with Multiple Seeds\n",
    "\n",
    "To confirm that the strong performance of our previous run and was not a fluke, we repeated the exact same setup—pretraining on MNIST with batch size 256 and adapting with batch size 256, fixed $\\alpha = 0.25$, and learning rate $1e^{-3}$—across three different random seeds (42, 100, and 2024)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 638685,
     "status": "ok",
     "timestamp": 1764666546183,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "-3it6tjcyXhr",
    "outputId": "2da38813-f30d-4600-a886-52a768224c17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Starting Robustness Experiment on 3 seeds...\n",
      "   Config: BS=256, LR=0.001, Alpha=0.25\n",
      "\n",
      "========================================\n",
      "  RUN 1/3 | SEED: 42\n",
      "========================================\n",
      "Seed set to 42\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.005 | Dom Loss: 0.194 | Dom Acc: 98.2%: 100%|██████████| 211/211 [00:08<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0207\n",
      "   Avg Domain Loss: 0.3631\n",
      "   Source Acc: 99.11%\n",
      "   Target Acc: 41.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.045 | Dom Loss: 0.092 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0203\n",
      "   Avg Domain Loss: 0.1288\n",
      "   Source Acc: 99.32%\n",
      "   Target Acc: 47.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.018 | Dom Loss: 0.095 | Dom Acc: 98.0%: 100%|██████████| 211/211 [00:08<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0197\n",
      "   Avg Domain Loss: 0.0830\n",
      "   Source Acc: 99.03%\n",
      "   Target Acc: 64.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.036 | Dom Loss: 0.105 | Dom Acc: 97.5%: 100%|██████████| 211/211 [00:08<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0195\n",
      "   Avg Domain Loss: 0.0763\n",
      "   Source Acc: 99.10%\n",
      "   Target Acc: 65.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.046 | Dom Loss: 0.147 | Dom Acc: 96.3%: 100%|██████████| 211/211 [00:08<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0261\n",
      "   Avg Domain Loss: 0.1111\n",
      "   Source Acc: 99.11%\n",
      "   Target Acc: 73.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.023 | Dom Loss: 0.322 | Dom Acc: 87.5%: 100%|██████████| 211/211 [00:08<00:00, 26.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0408\n",
      "   Avg Domain Loss: 0.2351\n",
      "   Source Acc: 99.15%\n",
      "   Target Acc: 68.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.128 | Dom Loss: 0.207 | Dom Acc: 92.2%: 100%|██████████| 211/211 [00:08<00:00, 25.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0682\n",
      "   Avg Domain Loss: 0.4642\n",
      "   Source Acc: 98.35%\n",
      "   Target Acc: 40.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.023 | Dom Loss: 0.131 | Dom Acc: 97.3%: 100%|██████████| 211/211 [00:08<00:00, 25.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0362\n",
      "   Avg Domain Loss: 0.1520\n",
      "   Source Acc: 99.19%\n",
      "   Target Acc: 62.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.033 | Dom Loss: 0.201 | Dom Acc: 92.6%: 100%|██████████| 211/211 [00:08<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0379\n",
      "   Avg Domain Loss: 0.2473\n",
      "   Source Acc: 98.69%\n",
      "   Target Acc: 64.79%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.040 | Dom Loss: 0.348 | Dom Acc: 88.3%: 100%|██████████| 211/211 [00:08<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0471\n",
      "   Avg Domain Loss: 0.3043\n",
      "   Source Acc: 99.08%\n",
      "   Target Acc: 78.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.022 | Dom Loss: 0.320 | Dom Acc: 88.9%: 100%|██████████| 211/211 [00:08<00:00, 26.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0387\n",
      "   Avg Domain Loss: 0.3153\n",
      "   Source Acc: 98.72%\n",
      "   Target Acc: 80.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.061 | Dom Loss: 0.441 | Dom Acc: 81.4%: 100%|██████████| 211/211 [00:08<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0476\n",
      "   Avg Domain Loss: 0.3847\n",
      "   Source Acc: 98.63%\n",
      "   Target Acc: 78.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.081 | Dom Loss: 0.438 | Dom Acc: 80.5%: 100%|██████████| 211/211 [00:08<00:00, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0467\n",
      "   Avg Domain Loss: 0.4165\n",
      "   Source Acc: 98.86%\n",
      "   Target Acc: 82.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.092 | Dom Loss: 0.483 | Dom Acc: 75.6%: 100%|██████████| 211/211 [00:08<00:00, 26.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0443\n",
      "   Avg Domain Loss: 0.4601\n",
      "   Source Acc: 98.48%\n",
      "   Target Acc: 73.10%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.052 | Dom Loss: 0.548 | Dom Acc: 72.1%: 100%|██████████| 211/211 [00:08<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0474\n",
      "   Avg Domain Loss: 0.4831\n",
      "   Source Acc: 98.72%\n",
      "   Target Acc: 82.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.023 | Dom Loss: 0.445 | Dom Acc: 80.3%: 100%|██████████| 211/211 [00:08<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0451\n",
      "   Avg Domain Loss: 0.5152\n",
      "   Source Acc: 98.96%\n",
      "   Target Acc: 85.67%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.082 | Dom Loss: 0.677 | Dom Acc: 63.5%: 100%|██████████| 211/211 [00:08<00:00, 25.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0446\n",
      "   Avg Domain Loss: 0.5066\n",
      "   Source Acc: 98.49%\n",
      "   Target Acc: 86.19%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.040 | Dom Loss: 0.599 | Dom Acc: 67.4%: 100%|██████████| 211/211 [00:08<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0431\n",
      "   Avg Domain Loss: 0.5485\n",
      "   Source Acc: 98.52%\n",
      "   Target Acc: 85.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.026 | Dom Loss: 0.652 | Dom Acc: 63.7%: 100%|██████████| 211/211 [00:08<00:00, 26.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0425\n",
      "   Avg Domain Loss: 0.5653\n",
      "   Source Acc: 98.82%\n",
      "   Target Acc: 86.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.066 | Dom Loss: 0.570 | Dom Acc: 70.7%: 100%|██████████| 211/211 [00:08<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0393\n",
      "   Avg Domain Loss: 0.5737\n",
      "   Source Acc: 99.12%\n",
      "   Target Acc: 85.33%\n",
      "\n",
      "Run 1 Complete. Best Acc: 86.58%\n",
      "\n",
      "========================================\n",
      "  RUN 2/3 | SEED: 100\n",
      "========================================\n",
      "Seed set to 100\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.045 | Dom Loss: 0.180 | Dom Acc: 99.8%: 100%|██████████| 211/211 [00:08<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0197\n",
      "   Avg Domain Loss: 0.3775\n",
      "   Source Acc: 99.22%\n",
      "   Target Acc: 43.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.024 | Dom Loss: 0.093 | Dom Acc: 99.8%: 100%|██████████| 211/211 [00:08<00:00, 25.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0186\n",
      "   Avg Domain Loss: 0.1234\n",
      "   Source Acc: 99.17%\n",
      "   Target Acc: 49.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.020 | Dom Loss: 0.075 | Dom Acc: 99.2%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0174\n",
      "   Avg Domain Loss: 0.0686\n",
      "   Source Acc: 99.37%\n",
      "   Target Acc: 55.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.014 | Dom Loss: 0.053 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 26.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0175\n",
      "   Avg Domain Loss: 0.0587\n",
      "   Source Acc: 99.35%\n",
      "   Target Acc: 60.37%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.012 | Dom Loss: 0.060 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:07<00:00, 26.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0203\n",
      "   Avg Domain Loss: 0.0590\n",
      "   Source Acc: 99.19%\n",
      "   Target Acc: 64.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.057 | Dom Loss: 0.236 | Dom Acc: 90.2%: 100%|██████████| 211/211 [00:08<00:00, 26.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0265\n",
      "   Avg Domain Loss: 0.1222\n",
      "   Source Acc: 98.92%\n",
      "   Target Acc: 58.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.020 | Dom Loss: 0.054 | Dom Acc: 99.4%: 100%|██████████| 211/211 [00:08<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0487\n",
      "   Avg Domain Loss: 0.2046\n",
      "   Source Acc: 99.24%\n",
      "   Target Acc: 34.28%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.047 | Dom Loss: 0.051 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0211\n",
      "   Avg Domain Loss: 0.0715\n",
      "   Source Acc: 99.29%\n",
      "   Target Acc: 61.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.051 | Dom Loss: 0.121 | Dom Acc: 97.1%: 100%|██████████| 211/211 [00:08<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0261\n",
      "   Avg Domain Loss: 0.1212\n",
      "   Source Acc: 99.10%\n",
      "   Target Acc: 69.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.060 | Dom Loss: 0.170 | Dom Acc: 95.3%: 100%|██████████| 211/211 [00:08<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0373\n",
      "   Avg Domain Loss: 0.1892\n",
      "   Source Acc: 99.12%\n",
      "   Target Acc: 72.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.019 | Dom Loss: 0.392 | Dom Acc: 82.8%: 100%|██████████| 211/211 [00:07<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0477\n",
      "   Avg Domain Loss: 0.3170\n",
      "   Source Acc: 98.82%\n",
      "   Target Acc: 79.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.027 | Dom Loss: 0.467 | Dom Acc: 80.3%: 100%|██████████| 211/211 [00:08<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0525\n",
      "   Avg Domain Loss: 0.3932\n",
      "   Source Acc: 98.73%\n",
      "   Target Acc: 79.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.114 | Dom Loss: 0.538 | Dom Acc: 75.4%: 100%|██████████| 211/211 [00:08<00:00, 25.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0527\n",
      "   Avg Domain Loss: 0.4709\n",
      "   Source Acc: 98.55%\n",
      "   Target Acc: 81.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.059 | Dom Loss: 0.527 | Dom Acc: 73.8%: 100%|██████████| 211/211 [00:08<00:00, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0529\n",
      "   Avg Domain Loss: 0.4981\n",
      "   Source Acc: 98.63%\n",
      "   Target Acc: 82.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.015 | Dom Loss: 0.418 | Dom Acc: 83.2%: 100%|██████████| 211/211 [00:08<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0516\n",
      "   Avg Domain Loss: 0.5090\n",
      "   Source Acc: 99.17%\n",
      "   Target Acc: 84.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.036 | Dom Loss: 0.592 | Dom Acc: 68.9%: 100%|██████████| 211/211 [00:08<00:00, 25.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0467\n",
      "   Avg Domain Loss: 0.5114\n",
      "   Source Acc: 98.52%\n",
      "   Target Acc: 84.12%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.067 | Dom Loss: 0.617 | Dom Acc: 68.0%: 100%|██████████| 211/211 [00:08<00:00, 25.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0433\n",
      "   Avg Domain Loss: 0.5367\n",
      "   Source Acc: 98.92%\n",
      "   Target Acc: 87.92%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.053 | Dom Loss: 0.423 | Dom Acc: 83.4%: 100%|██████████| 211/211 [00:07<00:00, 26.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0474\n",
      "   Avg Domain Loss: 0.5851\n",
      "   Source Acc: 98.78%\n",
      "   Target Acc: 80.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.027 | Dom Loss: 0.634 | Dom Acc: 67.8%: 100%|██████████| 211/211 [00:08<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0464\n",
      "   Avg Domain Loss: 0.6227\n",
      "   Source Acc: 98.75%\n",
      "   Target Acc: 84.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.013 | Dom Loss: 0.535 | Dom Acc: 73.0%: 100%|██████████| 211/211 [00:08<00:00, 25.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0390\n",
      "   Avg Domain Loss: 0.5946\n",
      "   Source Acc: 99.12%\n",
      "   Target Acc: 89.03%\n",
      "\n",
      "Run 2 Complete. Best Acc: 89.03%\n",
      "\n",
      "========================================\n",
      "  RUN 3/3 | SEED: 2024\n",
      "========================================\n",
      "Seed set to 2024\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.058 | Dom Loss: 0.201 | Dom Acc: 99.2%: 100%|██████████| 211/211 [00:08<00:00, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0200\n",
      "   Avg Domain Loss: 0.3617\n",
      "   Source Acc: 98.78%\n",
      "   Target Acc: 39.88%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.002 | Dom Loss: 0.105 | Dom Acc: 99.2%: 100%|██████████| 211/211 [00:08<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0214\n",
      "   Avg Domain Loss: 0.1245\n",
      "   Source Acc: 99.36%\n",
      "   Target Acc: 56.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.031 | Dom Loss: 0.085 | Dom Acc: 99.2%: 100%|██████████| 211/211 [00:08<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0205\n",
      "   Avg Domain Loss: 0.0890\n",
      "   Source Acc: 99.15%\n",
      "   Target Acc: 57.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.022 | Dom Loss: 0.052 | Dom Acc: 99.4%: 100%|██████████| 211/211 [00:08<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0217\n",
      "   Avg Domain Loss: 0.0791\n",
      "   Source Acc: 99.36%\n",
      "   Target Acc: 64.34%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.027 | Dom Loss: 0.186 | Dom Acc: 93.8%: 100%|██████████| 211/211 [00:08<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0216\n",
      "   Avg Domain Loss: 0.0938\n",
      "   Source Acc: 98.93%\n",
      "   Target Acc: 67.38%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.058 | Dom Loss: 0.267 | Dom Acc: 90.8%: 100%|██████████| 211/211 [00:08<00:00, 25.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0427\n",
      "   Avg Domain Loss: 0.2074\n",
      "   Source Acc: 99.05%\n",
      "   Target Acc: 73.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.049 | Dom Loss: 0.272 | Dom Acc: 90.8%: 100%|██████████| 211/211 [00:08<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0458\n",
      "   Avg Domain Loss: 0.3099\n",
      "   Source Acc: 98.96%\n",
      "   Target Acc: 80.08%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.035 | Dom Loss: 0.208 | Dom Acc: 93.6%: 100%|██████████| 211/211 [00:08<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0523\n",
      "   Avg Domain Loss: 0.3111\n",
      "   Source Acc: 99.00%\n",
      "   Target Acc: 62.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.122 | Dom Loss: 0.315 | Dom Acc: 88.7%: 100%|██████████| 211/211 [00:08<00:00, 25.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0433\n",
      "   Avg Domain Loss: 0.2629\n",
      "   Source Acc: 98.91%\n",
      "   Target Acc: 73.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.049 | Dom Loss: 0.444 | Dom Acc: 81.4%: 100%|██████████| 211/211 [00:08<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0502\n",
      "   Avg Domain Loss: 0.3714\n",
      "   Source Acc: 98.82%\n",
      "   Target Acc: 82.54%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.081 | Dom Loss: 0.411 | Dom Acc: 81.2%: 100%|██████████| 211/211 [00:07<00:00, 26.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0476\n",
      "   Avg Domain Loss: 0.4080\n",
      "   Source Acc: 98.98%\n",
      "   Target Acc: 82.36%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.025 | Dom Loss: 0.363 | Dom Acc: 88.1%: 100%|██████████| 211/211 [00:08<00:00, 25.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0430\n",
      "   Avg Domain Loss: 0.4444\n",
      "   Source Acc: 99.01%\n",
      "   Target Acc: 79.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.025 | Dom Loss: 0.456 | Dom Acc: 77.9%: 100%|██████████| 211/211 [00:08<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0437\n",
      "   Avg Domain Loss: 0.4821\n",
      "   Source Acc: 98.98%\n",
      "   Target Acc: 83.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.017 | Dom Loss: 0.419 | Dom Acc: 81.2%: 100%|██████████| 211/211 [00:08<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0428\n",
      "   Avg Domain Loss: 0.4701\n",
      "   Source Acc: 99.22%\n",
      "   Target Acc: 85.73%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.025 | Dom Loss: 0.467 | Dom Acc: 78.3%: 100%|██████████| 211/211 [00:08<00:00, 25.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0446\n",
      "   Avg Domain Loss: 0.5092\n",
      "   Source Acc: 98.61%\n",
      "   Target Acc: 87.33%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.035 | Dom Loss: 0.673 | Dom Acc: 63.5%: 100%|██████████| 211/211 [00:08<00:00, 26.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0420\n",
      "   Avg Domain Loss: 0.5216\n",
      "   Source Acc: 98.73%\n",
      "   Target Acc: 89.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.110 | Dom Loss: 0.577 | Dom Acc: 70.1%: 100%|██████████| 211/211 [00:08<00:00, 26.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0419\n",
      "   Avg Domain Loss: 0.5677\n",
      "   Source Acc: 99.09%\n",
      "   Target Acc: 91.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.042 | Dom Loss: 0.628 | Dom Acc: 67.6%: 100%|██████████| 211/211 [00:08<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0423\n",
      "   Avg Domain Loss: 0.5823\n",
      "   Source Acc: 98.87%\n",
      "   Target Acc: 88.43%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.032 | Dom Loss: 0.544 | Dom Acc: 70.9%: 100%|██████████| 211/211 [00:08<00:00, 25.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0429\n",
      "   Avg Domain Loss: 0.6009\n",
      "   Source Acc: 99.26%\n",
      "   Target Acc: 90.46%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.021 | Dom Loss: 0.614 | Dom Acc: 69.3%: 100%|██████████| 211/211 [00:08<00:00, 25.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0389\n",
      "   Avg Domain Loss: 0.5817\n",
      "   Source Acc: 99.13%\n",
      "   Target Acc: 89.52%\n",
      "\n",
      "Run 3 Complete. Best Acc: 91.51%\n",
      "\n",
      " ROBUSTNESS EXPERIMENT COMPLETE\n",
      "   Individual Runs: [86.57777777777778, 89.03333333333333, 91.5111111111111]\n",
      "   Final Result: 89.04% ± 2.01%\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [42, 100, 2024]\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "alpha = 0.25\n",
    "\n",
    "# Target location of our saved model\n",
    "models_path = \"/content/drive/MyDrive/DA_Project/models/\"\n",
    "best_model_file_path = os.path.join(models_path, f\"best_cnn_mnist_{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}.pth\")\n",
    "\n",
    "# Store final results\n",
    "final_accuracies = []\n",
    "\n",
    "print(f\"   Starting Robustness Experiment on {len(SEEDS)} seeds...\")\n",
    "print(f\"   Config: BS={BATCH_SIZE}, LR={LEARNING_RATE}, Alpha={alpha}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for run_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"  RUN {run_idx+1}/{len(SEEDS)} | SEED: {seed}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Re-Initialize DataLoaders\n",
    "    source_train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    source_test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    target_train_loader = DataLoader(mnistm_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    target_test_loader = DataLoader(mnistm_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "    # Initialize DANN\n",
    "    dann_model = DANN_CNN(num_classes=10).to(device)\n",
    "    if os.path.exists(best_model_file_path):\n",
    "        print(f\"Loading pre-trained source weights from {best_model_file_path}...\")\n",
    "        pretrained_state_dict = torch.load(best_model_file_path, map_location=device)\n",
    "\n",
    "        # Create a new state_dict for the DANN model, which we'll populate\n",
    "        dann_state_dict = dann_model.state_dict()\n",
    "\n",
    "        # Copy weights for the feature extractor\n",
    "        for k, v in pretrained_state_dict.items():\n",
    "            if k.startswith('features.'):\n",
    "                dann_state_dict[k] = v\n",
    "            elif k.startswith('classifier.'):\n",
    "                # Rename 'classifier' keys to 'class_classifier' to match DANN_CNN\n",
    "                new_key = k.replace('classifier.', 'class_classifier.')\n",
    "                dann_state_dict[new_key] = v\n",
    "\n",
    "        # Load the modified state_dict. strict=False is used because 'domain_classifier'\n",
    "        # keys will be missing from the loaded pretrained_state_dict, which is intended.\n",
    "        dann_model.load_state_dict(dann_state_dict, strict=False)\n",
    "        print(\"Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\\n\")\n",
    "    else:\n",
    "        print(\"Pre-trained weights not found. Starting DANN_CNN from scratch.\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(dann_model.parameters(), lr=LEARNING_RATE)\n",
    "    best_target_acc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        dann_model.train()\n",
    "\n",
    "        # Stats tracking\n",
    "        total_loss, total_domain_loss, total_class_loss = 0.0, 0.0, 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Zip loaders\n",
    "        min_len = min(len(source_train_loader), len(target_train_loader))\n",
    "        loader_zip = tqdm(zip(source_train_loader, target_train_loader), total=min_len)\n",
    "\n",
    "        for (source_imgs, source_labels), (target_imgs, _) in loader_zip:\n",
    "            # Setup\n",
    "            source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "            target_imgs = target_imgs.to(device)\n",
    "            batch_size = source_imgs.size(0)    # we do this once again to make sure of the batch size\n",
    "\n",
    "            # Handle uneven batches\n",
    "            if source_imgs.size(0) != target_imgs.size(0): continue\n",
    "\n",
    "            # Create Domain Labels: Source = 0, Target = 1\n",
    "            domain_label_s = torch.zeros(batch_size,1).float().to(device)\n",
    "            domain_label_t = torch.ones(batch_size,1).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass (Source Data)\n",
    "            # We calculate BOTH Class Loss and Domain Loss\n",
    "            class_logits_s, domain_logits_s = dann_model(source_imgs, alpha)\n",
    "            loss_class = criterion(class_logits_s, source_labels)\n",
    "            loss_domain_s = criterion_domain(domain_logits_s, domain_label_s)\n",
    "\n",
    "            # Forward Pass (Target Data)\n",
    "            # We ONLY calculate Domain Loss (we don't have class labels)\n",
    "            _, domain_logits_t = dann_model(target_imgs, alpha)\n",
    "            loss_domain_t = criterion_domain(domain_logits_t, domain_label_t)\n",
    "\n",
    "            # Backward Pass (Optimization)\n",
    "            # The GRL inside the model handles the sign flipping automatically!\n",
    "            loss_domain = (loss_domain_s + loss_domain_t)/2\n",
    "            loss = loss_class + loss_domain\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Stats\n",
    "            total_loss += loss.item()\n",
    "            total_class_loss += loss_class.item()\n",
    "            total_domain_loss += loss_domain.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prob_s = torch.sigmoid(domain_logits_s).mean().item()\n",
    "                prob_t = torch.sigmoid(domain_logits_t).mean().item()\n",
    "\n",
    "                # Accuracy checks\n",
    "\n",
    "                # Source is correctly classified if logit < 0 (Prob < 0.5)\n",
    "                acc_s = (domain_logits_s < 0).float().mean().item() * 100\n",
    "                # Target is correctly classified if logit > 0 (Prob > 0.5)\n",
    "                acc_t = (domain_logits_t > 0).float().mean().item() * 100\n",
    "\n",
    "                domain_acc = (acc_s + acc_t) / 2\n",
    "\n",
    "            # Logging\n",
    "            loader_zip.set_description(\n",
    "                f\"Ep {epoch+1} | Cls: {loss_class.item():.3f} | \"\n",
    "                f\"Dom Loss: {loss_domain.item():.3f} | \"\n",
    "                f\"Dom Acc: {domain_acc:.1f}%\"\n",
    "            )\n",
    "\n",
    "            # Average classification loss and domain loss\n",
    "            avg_class_loss = total_class_loss / total_samples\n",
    "            avg_domain_loss = total_domain_loss / total_samples\n",
    "\n",
    "        # Validation\n",
    "        # Evaluate on Target (Did adaptation work?)\n",
    "        target_acc = evaluate_accuracy(dann_model, target_test_loader, device)\n",
    "\n",
    "        # Evaluate on Source (Did we preserve original knowledge?)\n",
    "        source_acc = evaluate_accuracy(dann_model, source_test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Stats:\")\n",
    "        print(f\"   Avg CL Loss: {avg_class_loss:.4f}\")\n",
    "        print(f\"   Avg Domain Loss: {avg_domain_loss:.4f}\")\n",
    "        print(f\"   Source Acc: {source_acc:.2f}%\")\n",
    "        print(f\"   Target Acc: {target_acc:.2f}%\\n\")\n",
    "\n",
    "        # Checkpointing\n",
    "        if target_acc > best_target_acc:\n",
    "            best_target_acc = target_acc\n",
    "            best_model_wts = copy.deepcopy(dann_model.state_dict())\n",
    "\n",
    "    print(f\"Run {run_idx+1} Complete. Best Acc: {best_target_acc:.2f}%\")\n",
    "    final_accuracies.append(best_target_acc)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "mean_acc = np.mean(final_accuracies)\n",
    "std_acc = np.std(final_accuracies)\n",
    "print(f\"\\n ROBUSTNESS EXPERIMENT COMPLETE\")\n",
    "print(f\"   Individual Runs: {final_accuracies}\")\n",
    "print(f\"   Final Result: {mean_acc:.2f}% ± {std_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q75Vx8TFLEmG"
   },
   "source": [
    "The results were outstanding and validated the stability of this approach. Across the three runs, we achieved best target accuracies of **86.58%**, **89.03%**, and **91.51%**, resulting in a **mean of 89.04% ± 2.01%**. This performance is a decisive improvement over our internal distance-based (MMD) baseline (79.36%) and significantly outperforms the original DANN paper’s benchmark of 76.1% ± 1.8%. Source accuracy remained robust throughout (>98.5%), confirming that the adaptation did not degrade the primary task performance.\n",
    "\n",
    "Training dynamics consistently showed characteristic adversarial fluctuations followed by strong recovery. While target accuracy often dipped in early epochs (epochs 4–7), it reliably climbed into the high 80s by the later stages. The domain classifier accuracy settled between **69% and 77%** (with BCE loss $\\approx$ 0.57–0.60), rather than the theoretical optimum of 50%. This confirms our \"Sufficient Alignment\" hypothesis: the feature extractor successfully aligned the semantic digit features to drive state-of-the-art classification, even while retaining enough domain-specific noise to allow the discriminator to perform better than random guessing.\n",
    "\n",
    "This robustness check solidifies that matching batch statistics (256->256) combined with a weak fixed adversary is a highly effective and reproducible recipe for Domain Adaptation on MNIST-M.\n",
    "\n",
    "### Experiment 2: Stability Analysis — α and Learning Rate Annealing\n",
    "\n",
    "While Experiment 1 yielded high peak accuracy, the training dynamics exhibited significant instability, with target accuracy dropping as low as ~40% during epochs 4–7 before recovering. To mitigate this and test if a more gradual optimization path yields better convergence, we adopt the specific annealing schedules proposed in the original DANN paper.\n",
    "\n",
    "The objective is to introduce a \"warm-up\" phase where the model prioritizes learning basic semantic features before the adversarial penalty becomes significant. We implement two specific schedules:\n",
    "\n",
    "**1. Annealing $\\alpha$ (Adversarial Strength):**\n",
    "Instead of a fixed $\\alpha = 0.25$, we schedule the gradient reversal strength to ramp smoothly from 0 to 1.0 over the course of training:\n",
    "$$\n",
    "\\alpha_p = \\frac{2}{1 + \\exp(-10 \\cdot p)} - 1\n",
    "$$\n",
    "where $p \\in [0, 1]$ represents the training progress (current epoch / total epochs). This ensures early updates are effectively non-adversarial ($\\alpha \\approx 0$), allowing the feature extractor to stabilize, before ramping to full strength ($\\alpha \\to 1.0$) to force maximal domain confusion.\n",
    "\n",
    "**2. Annealing Learning Rate:**\n",
    "We implement the inverse decay schedule to gradually reduce the step size as training progresses:\n",
    "$$\n",
    "\\text{LR}_p = \\frac{\\text{LR}_0}{(1 + 10 \\cdot p)^{0.75}}\n",
    "$$\n",
    "where $\\text{LR}_0 = 10^{-3}$. This facilitates fast initial convergence while preventing the model from oscillating or \"over-correcting\" when the adversarial constraint becomes strong in later epochs.\n",
    "\n",
    "**Experimental Configuration:**\n",
    "* **Initialization:** Pre-trained Source Model (Batch 256)\n",
    "* **Batch Size:** 256\n",
    "* **Optimizer:** AdamW\n",
    "* **Learning Rate:** Annealed ($10^{-3} \\to \\sim 2\\times 10^{-4}$)\n",
    "* **Adversarial $\\alpha$:** Annealed ($0.0 \\to 1.0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 213109,
     "status": "ok",
     "timestamp": 1764667671455,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "i_V8lQl8L08p",
    "outputId": "46a98199-232d-4833-aa18-167f9acdea31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n",
      " Starting DANN Training (Adversarial)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.049 | Dom Loss: 0.085 | Dom Acc: 100.0%: 100%|██████████| 211/211 [00:08<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0159\n",
      "   Avg Domain Loss: 0.3140\n",
      "   Source Acc: 98.76%\n",
      "   Target Acc: 50.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.024 | Dom Loss: 0.113 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0167\n",
      "   Avg Domain Loss: 0.1193\n",
      "   Source Acc: 99.39%\n",
      "   Target Acc: 44.69%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.006 | Dom Loss: 0.131 | Dom Acc: 98.4%: 100%|██████████| 211/211 [00:08<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0279\n",
      "   Avg Domain Loss: 0.1485\n",
      "   Source Acc: 99.18%\n",
      "   Target Acc: 49.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.081 | Dom Loss: 0.345 | Dom Acc: 86.1%: 100%|██████████| 211/211 [00:08<00:00, 25.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0619\n",
      "   Avg Domain Loss: 0.2634\n",
      "   Source Acc: 98.14%\n",
      "   Target Acc: 56.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.073 | Dom Loss: 0.373 | Dom Acc: 86.7%: 100%|██████████| 211/211 [00:08<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0914\n",
      "   Avg Domain Loss: 0.4556\n",
      "   Source Acc: 98.62%\n",
      "   Target Acc: 69.18%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.122 | Dom Loss: 0.515 | Dom Acc: 75.8%: 100%|██████████| 211/211 [00:08<00:00, 26.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0877\n",
      "   Avg Domain Loss: 0.4649\n",
      "   Source Acc: 98.07%\n",
      "   Target Acc: 74.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.137 | Dom Loss: 0.638 | Dom Acc: 67.2%: 100%|██████████| 211/211 [00:08<00:00, 26.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0925\n",
      "   Avg Domain Loss: 0.5228\n",
      "   Source Acc: 96.63%\n",
      "   Target Acc: 66.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.086 | Dom Loss: 0.536 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:08<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0920\n",
      "   Avg Domain Loss: 0.5370\n",
      "   Source Acc: 98.15%\n",
      "   Target Acc: 75.31%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.106 | Dom Loss: 0.486 | Dom Acc: 79.5%: 100%|██████████| 211/211 [00:08<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0857\n",
      "   Avg Domain Loss: 0.5234\n",
      "   Source Acc: 98.67%\n",
      "   Target Acc: 78.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.108 | Dom Loss: 0.508 | Dom Acc: 77.7%: 100%|██████████| 211/211 [00:08<00:00, 25.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0811\n",
      "   Avg Domain Loss: 0.4973\n",
      "   Source Acc: 98.59%\n",
      "   Target Acc: 82.02%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.164 | Dom Loss: 0.512 | Dom Acc: 77.1%: 100%|██████████| 211/211 [00:08<00:00, 26.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0771\n",
      "   Avg Domain Loss: 0.4960\n",
      "   Source Acc: 96.60%\n",
      "   Target Acc: 65.17%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.066 | Dom Loss: 0.534 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:07<00:00, 26.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0899\n",
      "   Avg Domain Loss: 0.5444\n",
      "   Source Acc: 98.20%\n",
      "   Target Acc: 79.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.036 | Dom Loss: 0.526 | Dom Acc: 75.0%: 100%|██████████| 211/211 [00:08<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0765\n",
      "   Avg Domain Loss: 0.5338\n",
      "   Source Acc: 98.23%\n",
      "   Target Acc: 79.11%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.159 | Dom Loss: 0.601 | Dom Acc: 67.8%: 100%|██████████| 211/211 [00:08<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0823\n",
      "   Avg Domain Loss: 0.5437\n",
      "   Source Acc: 97.66%\n",
      "   Target Acc: 78.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.062 | Dom Loss: 0.537 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:08<00:00, 25.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0713\n",
      "   Avg Domain Loss: 0.5137\n",
      "   Source Acc: 98.84%\n",
      "   Target Acc: 75.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.064 | Dom Loss: 0.550 | Dom Acc: 70.7%: 100%|██████████| 211/211 [00:08<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0698\n",
      "   Avg Domain Loss: 0.5350\n",
      "   Source Acc: 97.85%\n",
      "   Target Acc: 76.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.085 | Dom Loss: 0.656 | Dom Acc: 64.5%: 100%|██████████| 211/211 [00:08<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0829\n",
      "   Avg Domain Loss: 0.5628\n",
      "   Source Acc: 98.31%\n",
      "   Target Acc: 76.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.152 | Dom Loss: 0.601 | Dom Acc: 70.3%: 100%|██████████| 211/211 [00:08<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0825\n",
      "   Avg Domain Loss: 0.5694\n",
      "   Source Acc: 97.80%\n",
      "   Target Acc: 74.96%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.080 | Dom Loss: 0.569 | Dom Acc: 68.8%: 100%|██████████| 211/211 [00:08<00:00, 26.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0746\n",
      "   Avg Domain Loss: 0.5292\n",
      "   Source Acc: 98.15%\n",
      "   Target Acc: 81.26%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.166 | Dom Loss: 0.562 | Dom Acc: 71.9%: 100%|██████████| 211/211 [00:08<00:00, 26.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0857\n",
      "   Avg Domain Loss: 0.5701\n",
      "   Source Acc: 97.86%\n",
      "   Target Acc: 75.03%\n",
      "\n",
      "Best model weights saved to /content/drive/MyDrive/DA_Project/models/best_DANN_Exp2256_20_0.001annealed_1.0annealed.pth\n",
      "\n",
      "Adaptation Complete in 212.9s Best Target Accuracy: 82.02%\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "max_alpha = 1.0\n",
    "\n",
    "# Target location of our saved model\n",
    "models_path = \"/content/drive/MyDrive/DA_Project/models/\"\n",
    "best_model_file_path = os.path.join(models_path, f\"best_cnn_mnist_{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}.pth\")\n",
    "\n",
    "# Initialize DANN\n",
    "dann_model = DANN_CNN(num_classes=10).to(device)\n",
    "if os.path.exists(best_model_file_path):\n",
    "    print(f\"Loading pre-trained source weights from {best_model_file_path}...\")\n",
    "    pretrained_state_dict = torch.load(best_model_file_path, map_location=device)\n",
    "\n",
    "    # Create a new state_dict for the DANN model, which we'll populate\n",
    "    dann_state_dict = dann_model.state_dict()\n",
    "\n",
    "    # Copy weights for the feature extractor\n",
    "    for k, v in pretrained_state_dict.items():\n",
    "        if k.startswith('features.'):\n",
    "            dann_state_dict[k] = v\n",
    "        elif k.startswith('classifier.'):\n",
    "            # Rename 'classifier' keys to 'class_classifier' to match DANN_CNN\n",
    "            new_key = k.replace('classifier.', 'class_classifier.')\n",
    "            dann_state_dict[new_key] = v\n",
    "\n",
    "    # Load the modified state_dict. strict=False is used because 'domain_classifier'\n",
    "    # keys will be missing from the loaded pretrained_state_dict, which is intended.\n",
    "    dann_model.load_state_dict(dann_state_dict, strict=False)\n",
    "    print(\"Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\\n\")\n",
    "else:\n",
    "    print(\"Pre-trained weights not found. Starting DANN_CNN from scratch.\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_domain = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.AdamW(dann_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "best_target_acc = 0.0\n",
    "\n",
    "print(f\" Starting DANN Training (Adversarial)...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    dann_model.train()\n",
    "\n",
    "    # Stats tracking\n",
    "    total_loss, total_domain_loss, total_class_loss = 0.0, 0.0, 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Schedule alpha\n",
    "    progress = epoch / NUM_EPOCHS\n",
    "    annealing_factor = 2 / (1 + np.exp(-10 * progress)) - 1\n",
    "    alpha = max_alpha * annealing_factor\n",
    "\n",
    "    # Schedule learning rate\n",
    "    NEW_LEARNING_RATE = LEARNING_RATE / ((1 + 10 * progress) ** 0.75)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = NEW_LEARNING_RATE\n",
    "\n",
    "    # Zip loaders\n",
    "    min_len = min(len(source_train_loader), len(target_train_loader))\n",
    "    loader_zip = tqdm(zip(source_train_loader, target_train_loader), total=min_len)\n",
    "\n",
    "    for (source_imgs, source_labels), (target_imgs, _) in loader_zip:\n",
    "        # Setup\n",
    "        source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "        target_imgs = target_imgs.to(device)\n",
    "        batch_size = source_imgs.size(0)    # we do this once again to make sure of the batch size\n",
    "\n",
    "        # Handle uneven batches\n",
    "        if source_imgs.size(0) != target_imgs.size(0): continue\n",
    "\n",
    "        # Create Domain Labels: Source = 0, Target = 1\n",
    "        domain_label_s = torch.zeros(batch_size,1).float().to(device)\n",
    "        domain_label_t = torch.ones(batch_size,1).float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward Pass (Source Data)\n",
    "        # We calculate BOTH Class Loss and Domain Loss\n",
    "        class_logits_s, domain_logits_s = dann_model(source_imgs, alpha)\n",
    "        loss_class = criterion(class_logits_s, source_labels)\n",
    "        loss_domain_s = criterion_domain(domain_logits_s, domain_label_s)\n",
    "\n",
    "        # Forward Pass (Target Data)\n",
    "        # We ONLY calculate Domain Loss (we don't have class labels)\n",
    "        _, domain_logits_t = dann_model(target_imgs, alpha)\n",
    "        loss_domain_t = criterion_domain(domain_logits_t, domain_label_t)\n",
    "\n",
    "        # Backward Pass (Optimization)\n",
    "        # The GRL inside the model handles the sign flipping automatically!\n",
    "        loss_domain = (loss_domain_s + loss_domain_t)/2\n",
    "        loss = loss_class + loss_domain\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Stats\n",
    "        total_loss += loss.item()\n",
    "        total_class_loss += loss_class.item()\n",
    "        total_domain_loss += loss_domain.item()\n",
    "        total_samples += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob_s = torch.sigmoid(domain_logits_s).mean().item()\n",
    "            prob_t = torch.sigmoid(domain_logits_t).mean().item()\n",
    "\n",
    "            # Accuracy checks\n",
    "\n",
    "            # Source is correctly classified if logit < 0 (Prob < 0.5)\n",
    "            acc_s = (domain_logits_s < 0).float().mean().item() * 100\n",
    "            # Target is correctly classified if logit > 0 (Prob > 0.5)\n",
    "            acc_t = (domain_logits_t > 0).float().mean().item() * 100\n",
    "\n",
    "            domain_acc = (acc_s + acc_t) / 2\n",
    "\n",
    "        # Logging\n",
    "        loader_zip.set_description(\n",
    "            f\"Ep {epoch+1} | Cls: {loss_class.item():.3f} | \"\n",
    "            f\"Dom Loss: {loss_domain.item():.3f} | \"\n",
    "            f\"Dom Acc: {domain_acc:.1f}%\"\n",
    "        )\n",
    "\n",
    "        # Average classification loss and domain loss\n",
    "        avg_class_loss = total_class_loss / total_samples\n",
    "        avg_domain_loss = total_domain_loss / total_samples\n",
    "\n",
    "    # Validation\n",
    "    # Evaluate on Target (Did adaptation work?)\n",
    "    target_acc = evaluate_accuracy(dann_model, target_test_loader, device)\n",
    "\n",
    "    # Evaluate on Source (Did we preserve original knowledge?)\n",
    "    source_acc = evaluate_accuracy(dann_model, source_test_loader, device)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Stats:\")\n",
    "    print(f\"   Avg CL Loss: {avg_class_loss:.4f}\")\n",
    "    print(f\"   Avg Domain Loss: {avg_domain_loss:.4f}\")\n",
    "    print(f\"   Source Acc: {source_acc:.2f}%\")\n",
    "    print(f\"   Target Acc: {target_acc:.2f}%\\n\")\n",
    "\n",
    "    # Checkpointing\n",
    "    if target_acc > best_target_acc:\n",
    "        best_target_acc = target_acc\n",
    "        best_model_wts = copy.deepcopy(dann_model.state_dict())\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "# Save the best model weights\n",
    "target_file_path_best = os.path.join(models_path, f\"best_DANN_Exp2{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}annealed_{max_alpha}annealed.pth\")\n",
    "torch.save(best_model_wts, target_file_path_best)\n",
    "print(f\"Best model weights saved to {target_file_path_best}\")\n",
    "print(f\"\\nAdaptation Complete in {total_time:.1f}s Best Target Accuracy: {best_target_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNPUkcA1kgwL"
   },
   "source": [
    "### Takeaway Experiment 2: Annealing for Stability\n",
    "\n",
    "In this experiment, we adopted the standard DANN optimization strategy: initializing with Batch 256 weights and annealing the adversarial strength $\\alpha$ from 0 to 1, coupled with a decaying learning rate. This configuration achieved a best target accuracy of 82.02%.\n",
    "While this performance confirms the effectiveness of adversarial adaptation—surpassing the 79.36% distance-based benchmark—it falls significantly short of the ~89% achieved by our 'Fixed Weak Alpha' strategy. These results suggest that while the annealing 'warm-up' is beneficial in early epochs, allowing the adversarial strength to ramp up to $\\alpha \\approx 1.0$ forces the discriminator to become too aggressive. This over-correction appears to degrade the semantic features, confirming that for MNIST-M, a constant, gentle adversarial pressure ($\\alpha=0.25$) yields better generalization than the theoretically standard schedule of forcing perfect domain confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 643500,
     "status": "ok",
     "timestamp": 1764667441163,
     "user": {
      "displayName": "Chakradhar Rangi",
      "userId": "16472046945454388133"
     },
     "user_tz": 360
    },
    "id": "PjIPSeScQgEk",
    "outputId": "ccd9e837-c5bf-4ba4-9fa3-2bbac0e7c50e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Starting Robustness Experiment on 3 seeds...\n",
      "   Config: BS=256, LR=0.001, Alpha=0.24996257688624474\n",
      "\n",
      "========================================\n",
      "  RUN 1/3 | SEED: 42\n",
      "========================================\n",
      "Seed set to 42\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.002 | Dom Loss: 0.076 | Dom Acc: 100.0%: 100%|██████████| 211/211 [00:08<00:00, 25.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0177\n",
      "   Avg Domain Loss: 0.2931\n",
      "   Source Acc: 99.31%\n",
      "   Target Acc: 49.29%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.041 | Dom Loss: 0.089 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0150\n",
      "   Avg Domain Loss: 0.1151\n",
      "   Source Acc: 99.23%\n",
      "   Target Acc: 44.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.017 | Dom Loss: 0.256 | Dom Acc: 93.9%: 100%|██████████| 211/211 [00:08<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0325\n",
      "   Avg Domain Loss: 0.1703\n",
      "   Source Acc: 98.70%\n",
      "   Target Acc: 58.84%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.039 | Dom Loss: 0.215 | Dom Acc: 94.9%: 100%|██████████| 211/211 [00:08<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0794\n",
      "   Avg Domain Loss: 0.3658\n",
      "   Source Acc: 99.08%\n",
      "   Target Acc: 48.74%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.097 | Dom Loss: 0.366 | Dom Acc: 88.7%: 100%|██████████| 211/211 [00:08<00:00, 26.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0848\n",
      "   Avg Domain Loss: 0.3714\n",
      "   Source Acc: 98.55%\n",
      "   Target Acc: 63.27%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.079 | Dom Loss: 0.427 | Dom Acc: 83.2%: 100%|██████████| 211/211 [00:08<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0879\n",
      "   Avg Domain Loss: 0.4536\n",
      "   Source Acc: 98.08%\n",
      "   Target Acc: 71.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.132 | Dom Loss: 0.444 | Dom Acc: 84.2%: 100%|██████████| 211/211 [00:08<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0935\n",
      "   Avg Domain Loss: 0.4899\n",
      "   Source Acc: 98.87%\n",
      "   Target Acc: 71.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.130 | Dom Loss: 0.593 | Dom Acc: 69.5%: 100%|██████████| 211/211 [00:08<00:00, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0906\n",
      "   Avg Domain Loss: 0.4965\n",
      "   Source Acc: 98.45%\n",
      "   Target Acc: 67.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.024 | Dom Loss: 0.481 | Dom Acc: 81.1%: 100%|██████████| 211/211 [00:08<00:00, 26.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0862\n",
      "   Avg Domain Loss: 0.5000\n",
      "   Source Acc: 98.23%\n",
      "   Target Acc: 72.17%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.033 | Dom Loss: 0.464 | Dom Acc: 80.7%: 100%|██████████| 211/211 [00:08<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0859\n",
      "   Avg Domain Loss: 0.5127\n",
      "   Source Acc: 98.86%\n",
      "   Target Acc: 74.17%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.058 | Dom Loss: 0.544 | Dom Acc: 73.8%: 100%|██████████| 211/211 [00:07<00:00, 26.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0890\n",
      "   Avg Domain Loss: 0.5484\n",
      "   Source Acc: 98.32%\n",
      "   Target Acc: 75.53%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.101 | Dom Loss: 0.490 | Dom Acc: 76.8%: 100%|██████████| 211/211 [00:08<00:00, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0869\n",
      "   Avg Domain Loss: 0.5054\n",
      "   Source Acc: 98.48%\n",
      "   Target Acc: 68.52%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.102 | Dom Loss: 0.567 | Dom Acc: 70.9%: 100%|██████████| 211/211 [00:08<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0912\n",
      "   Avg Domain Loss: 0.5713\n",
      "   Source Acc: 98.52%\n",
      "   Target Acc: 76.90%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.073 | Dom Loss: 0.500 | Dom Acc: 79.3%: 100%|██████████| 211/211 [00:08<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0830\n",
      "   Avg Domain Loss: 0.5396\n",
      "   Source Acc: 98.83%\n",
      "   Target Acc: 76.87%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.091 | Dom Loss: 0.590 | Dom Acc: 69.1%: 100%|██████████| 211/211 [00:08<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0850\n",
      "   Avg Domain Loss: 0.5360\n",
      "   Source Acc: 98.81%\n",
      "   Target Acc: 74.94%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.064 | Dom Loss: 0.541 | Dom Acc: 73.6%: 100%|██████████| 211/211 [00:08<00:00, 26.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0819\n",
      "   Avg Domain Loss: 0.5506\n",
      "   Source Acc: 98.17%\n",
      "   Target Acc: 72.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.074 | Dom Loss: 0.545 | Dom Acc: 75.4%: 100%|██████████| 211/211 [00:08<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0779\n",
      "   Avg Domain Loss: 0.5190\n",
      "   Source Acc: 98.83%\n",
      "   Target Acc: 74.03%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.057 | Dom Loss: 0.428 | Dom Acc: 87.7%: 100%|██████████| 211/211 [00:08<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0694\n",
      "   Avg Domain Loss: 0.5069\n",
      "   Source Acc: 98.95%\n",
      "   Target Acc: 80.01%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.030 | Dom Loss: 0.508 | Dom Acc: 77.9%: 100%|██████████| 211/211 [00:08<00:00, 25.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0672\n",
      "   Avg Domain Loss: 0.5067\n",
      "   Source Acc: 98.80%\n",
      "   Target Acc: 84.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.103 | Dom Loss: 0.551 | Dom Acc: 74.4%: 100%|██████████| 211/211 [00:08<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0775\n",
      "   Avg Domain Loss: 0.5545\n",
      "   Source Acc: 98.51%\n",
      "   Target Acc: 80.21%\n",
      "\n",
      "Run 1 Complete. Best Acc: 84.70%\n",
      "\n",
      "========================================\n",
      "  RUN 2/3 | SEED: 100\n",
      "========================================\n",
      "Seed set to 100\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.035 | Dom Loss: 0.088 | Dom Acc: 100.0%: 100%|██████████| 211/211 [00:08<00:00, 24.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0160\n",
      "   Avg Domain Loss: 0.3219\n",
      "   Source Acc: 98.79%\n",
      "   Target Acc: 49.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.064 | Dom Loss: 0.102 | Dom Acc: 99.6%: 100%|██████████| 211/211 [00:08<00:00, 25.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0144\n",
      "   Avg Domain Loss: 0.1235\n",
      "   Source Acc: 98.84%\n",
      "   Target Acc: 40.21%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.072 | Dom Loss: 0.224 | Dom Acc: 94.3%: 100%|██████████| 211/211 [00:08<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0267\n",
      "   Avg Domain Loss: 0.1324\n",
      "   Source Acc: 98.28%\n",
      "   Target Acc: 50.43%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.058 | Dom Loss: 0.396 | Dom Acc: 82.2%: 100%|██████████| 211/211 [00:08<00:00, 26.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0736\n",
      "   Avg Domain Loss: 0.3274\n",
      "   Source Acc: 98.30%\n",
      "   Target Acc: 62.16%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.030 | Dom Loss: 0.451 | Dom Acc: 80.3%: 100%|██████████| 211/211 [00:08<00:00, 25.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0831\n",
      "   Avg Domain Loss: 0.4558\n",
      "   Source Acc: 98.67%\n",
      "   Target Acc: 67.11%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.121 | Dom Loss: 0.606 | Dom Acc: 69.3%: 100%|██████████| 211/211 [00:08<00:00, 25.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0873\n",
      "   Avg Domain Loss: 0.5046\n",
      "   Source Acc: 97.70%\n",
      "   Target Acc: 65.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.063 | Dom Loss: 0.469 | Dom Acc: 78.3%: 100%|██████████| 211/211 [00:08<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0825\n",
      "   Avg Domain Loss: 0.5194\n",
      "   Source Acc: 98.41%\n",
      "   Target Acc: 72.08%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.054 | Dom Loss: 0.470 | Dom Acc: 80.5%: 100%|██████████| 211/211 [00:08<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0861\n",
      "   Avg Domain Loss: 0.5347\n",
      "   Source Acc: 98.73%\n",
      "   Target Acc: 72.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.067 | Dom Loss: 0.474 | Dom Acc: 80.9%: 100%|██████████| 211/211 [00:08<00:00, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0769\n",
      "   Avg Domain Loss: 0.4991\n",
      "   Source Acc: 98.30%\n",
      "   Target Acc: 73.71%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.091 | Dom Loss: 0.506 | Dom Acc: 77.9%: 100%|██████████| 211/211 [00:08<00:00, 25.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0814\n",
      "   Avg Domain Loss: 0.5223\n",
      "   Source Acc: 98.66%\n",
      "   Target Acc: 72.97%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.055 | Dom Loss: 0.572 | Dom Acc: 70.3%: 100%|██████████| 211/211 [00:08<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0778\n",
      "   Avg Domain Loss: 0.5386\n",
      "   Source Acc: 98.59%\n",
      "   Target Acc: 75.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.088 | Dom Loss: 0.576 | Dom Acc: 70.9%: 100%|██████████| 211/211 [00:08<00:00, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0797\n",
      "   Avg Domain Loss: 0.5247\n",
      "   Source Acc: 98.46%\n",
      "   Target Acc: 73.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.108 | Dom Loss: 0.652 | Dom Acc: 63.1%: 100%|██████████| 211/211 [00:08<00:00, 25.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0835\n",
      "   Avg Domain Loss: 0.5648\n",
      "   Source Acc: 98.32%\n",
      "   Target Acc: 71.76%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.062 | Dom Loss: 0.498 | Dom Acc: 78.9%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0771\n",
      "   Avg Domain Loss: 0.5636\n",
      "   Source Acc: 98.80%\n",
      "   Target Acc: 73.40%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.024 | Dom Loss: 0.530 | Dom Acc: 73.8%: 100%|██████████| 211/211 [00:08<00:00, 25.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0686\n",
      "   Avg Domain Loss: 0.5142\n",
      "   Source Acc: 98.91%\n",
      "   Target Acc: 77.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.039 | Dom Loss: 0.590 | Dom Acc: 69.9%: 100%|██████████| 211/211 [00:08<00:00, 26.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0649\n",
      "   Avg Domain Loss: 0.5551\n",
      "   Source Acc: 98.39%\n",
      "   Target Acc: 78.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.086 | Dom Loss: 0.542 | Dom Acc: 73.4%: 100%|██████████| 211/211 [00:07<00:00, 26.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0782\n",
      "   Avg Domain Loss: 0.5625\n",
      "   Source Acc: 98.47%\n",
      "   Target Acc: 77.24%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.134 | Dom Loss: 0.518 | Dom Acc: 75.6%: 100%|██████████| 211/211 [00:08<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0625\n",
      "   Avg Domain Loss: 0.5191\n",
      "   Source Acc: 98.69%\n",
      "   Target Acc: 77.41%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.042 | Dom Loss: 0.580 | Dom Acc: 68.6%: 100%|██████████| 211/211 [00:08<00:00, 25.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0652\n",
      "   Avg Domain Loss: 0.5383\n",
      "   Source Acc: 98.24%\n",
      "   Target Acc: 79.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.078 | Dom Loss: 0.565 | Dom Acc: 71.9%: 100%|██████████| 211/211 [00:08<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0679\n",
      "   Avg Domain Loss: 0.5452\n",
      "   Source Acc: 98.44%\n",
      "   Target Acc: 80.94%\n",
      "\n",
      "Run 2 Complete. Best Acc: 80.94%\n",
      "\n",
      "========================================\n",
      "  RUN 3/3 | SEED: 2024\n",
      "========================================\n",
      "Seed set to 2024\n",
      "Loading pre-trained source weights from /content/drive/MyDrive/DA_Project/models/best_cnn_mnist_256_20_0.001.pth...\n",
      "Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 1 | Cls: 0.011 | Dom Loss: 0.076 | Dom Acc: 100.0%: 100%|██████████| 211/211 [00:08<00:00, 25.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Stats:\n",
      "   Avg CL Loss: 0.0159\n",
      "   Avg Domain Loss: 0.2987\n",
      "   Source Acc: 99.31%\n",
      "   Target Acc: 51.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 2 | Cls: 0.003 | Dom Loss: 0.078 | Dom Acc: 100.0%: 100%|██████████| 211/211 [00:08<00:00, 25.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20] Stats:\n",
      "   Avg CL Loss: 0.0165\n",
      "   Avg Domain Loss: 0.1113\n",
      "   Source Acc: 99.45%\n",
      "   Target Acc: 33.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 3 | Cls: 0.029 | Dom Loss: 0.146 | Dom Acc: 98.0%: 100%|██████████| 211/211 [00:07<00:00, 26.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20] Stats:\n",
      "   Avg CL Loss: 0.0226\n",
      "   Avg Domain Loss: 0.1132\n",
      "   Source Acc: 99.23%\n",
      "   Target Acc: 59.42%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 4 | Cls: 0.032 | Dom Loss: 0.233 | Dom Acc: 94.3%: 100%|██████████| 211/211 [00:08<00:00, 25.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20] Stats:\n",
      "   Avg CL Loss: 0.0713\n",
      "   Avg Domain Loss: 0.2890\n",
      "   Source Acc: 98.81%\n",
      "   Target Acc: 56.98%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 5 | Cls: 0.137 | Dom Loss: 0.476 | Dom Acc: 80.1%: 100%|██████████| 211/211 [00:08<00:00, 24.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20] Stats:\n",
      "   Avg CL Loss: 0.0901\n",
      "   Avg Domain Loss: 0.4149\n",
      "   Source Acc: 98.22%\n",
      "   Target Acc: 65.64%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 6 | Cls: 0.088 | Dom Loss: 0.438 | Dom Acc: 83.8%: 100%|██████████| 211/211 [00:08<00:00, 24.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20] Stats:\n",
      "   Avg CL Loss: 0.0994\n",
      "   Avg Domain Loss: 0.4835\n",
      "   Source Acc: 98.85%\n",
      "   Target Acc: 78.43%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 7 | Cls: 0.130 | Dom Loss: 0.530 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:08<00:00, 24.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20] Stats:\n",
      "   Avg CL Loss: 0.0890\n",
      "   Avg Domain Loss: 0.5038\n",
      "   Source Acc: 97.74%\n",
      "   Target Acc: 69.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 8 | Cls: 0.070 | Dom Loss: 0.496 | Dom Acc: 79.3%: 100%|██████████| 211/211 [00:08<00:00, 24.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20] Stats:\n",
      "   Avg CL Loss: 0.0813\n",
      "   Avg Domain Loss: 0.5160\n",
      "   Source Acc: 98.54%\n",
      "   Target Acc: 80.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 9 | Cls: 0.092 | Dom Loss: 0.445 | Dom Acc: 81.4%: 100%|██████████| 211/211 [00:08<00:00, 25.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20] Stats:\n",
      "   Avg CL Loss: 0.0986\n",
      "   Avg Domain Loss: 0.5518\n",
      "   Source Acc: 98.45%\n",
      "   Target Acc: 72.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 10 | Cls: 0.051 | Dom Loss: 0.509 | Dom Acc: 77.1%: 100%|██████████| 211/211 [00:08<00:00, 25.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20] Stats:\n",
      "   Avg CL Loss: 0.0917\n",
      "   Avg Domain Loss: 0.5428\n",
      "   Source Acc: 98.77%\n",
      "   Target Acc: 74.02%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 11 | Cls: 0.055 | Dom Loss: 0.450 | Dom Acc: 83.0%: 100%|██████████| 211/211 [00:08<00:00, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20] Stats:\n",
      "   Avg CL Loss: 0.0744\n",
      "   Avg Domain Loss: 0.5023\n",
      "   Source Acc: 98.75%\n",
      "   Target Acc: 78.19%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 12 | Cls: 0.021 | Dom Loss: 0.498 | Dom Acc: 79.1%: 100%|██████████| 211/211 [00:08<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20] Stats:\n",
      "   Avg CL Loss: 0.0817\n",
      "   Avg Domain Loss: 0.5527\n",
      "   Source Acc: 98.89%\n",
      "   Target Acc: 79.63%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 13 | Cls: 0.057 | Dom Loss: 0.558 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:08<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20] Stats:\n",
      "   Avg CL Loss: 0.0765\n",
      "   Avg Domain Loss: 0.5234\n",
      "   Source Acc: 98.67%\n",
      "   Target Acc: 73.63%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 14 | Cls: 0.105 | Dom Loss: 0.510 | Dom Acc: 76.8%: 100%|██████████| 211/211 [00:08<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20] Stats:\n",
      "   Avg CL Loss: 0.0801\n",
      "   Avg Domain Loss: 0.5251\n",
      "   Source Acc: 98.13%\n",
      "   Target Acc: 74.91%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 15 | Cls: 0.132 | Dom Loss: 0.490 | Dom Acc: 78.1%: 100%|██████████| 211/211 [00:08<00:00, 25.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20] Stats:\n",
      "   Avg CL Loss: 0.0898\n",
      "   Avg Domain Loss: 0.5474\n",
      "   Source Acc: 98.26%\n",
      "   Target Acc: 72.24%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 16 | Cls: 0.109 | Dom Loss: 0.506 | Dom Acc: 76.8%: 100%|██████████| 211/211 [00:08<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20] Stats:\n",
      "   Avg CL Loss: 0.0858\n",
      "   Avg Domain Loss: 0.5522\n",
      "   Source Acc: 96.91%\n",
      "   Target Acc: 71.49%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 17 | Cls: 0.135 | Dom Loss: 0.557 | Dom Acc: 73.6%: 100%|██████████| 211/211 [00:07<00:00, 26.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20] Stats:\n",
      "   Avg CL Loss: 0.0750\n",
      "   Avg Domain Loss: 0.5035\n",
      "   Source Acc: 98.03%\n",
      "   Target Acc: 78.20%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 18 | Cls: 0.042 | Dom Loss: 0.530 | Dom Acc: 74.6%: 100%|██████████| 211/211 [00:08<00:00, 26.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20] Stats:\n",
      "   Avg CL Loss: 0.0767\n",
      "   Avg Domain Loss: 0.5471\n",
      "   Source Acc: 98.50%\n",
      "   Target Acc: 71.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 19 | Cls: 0.052 | Dom Loss: 0.535 | Dom Acc: 75.2%: 100%|██████████| 211/211 [00:08<00:00, 25.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20] Stats:\n",
      "   Avg CL Loss: 0.0778\n",
      "   Avg Domain Loss: 0.5444\n",
      "   Source Acc: 98.34%\n",
      "   Target Acc: 70.93%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep 20 | Cls: 0.074 | Dom Loss: 0.539 | Dom Acc: 74.2%: 100%|██████████| 211/211 [00:08<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20] Stats:\n",
      "   Avg CL Loss: 0.0886\n",
      "   Avg Domain Loss: 0.5480\n",
      "   Source Acc: 98.53%\n",
      "   Target Acc: 72.73%\n",
      "\n",
      "Run 3 Complete. Best Acc: 80.66%\n",
      "\n",
      " ROBUSTNESS EXPERIMENT COMPLETE WITH ANNEALING\n",
      "   Individual Runs: [84.7, 80.94444444444444, 80.65555555555555]\n",
      "   Final Result: 82.10% ± 1.84%\n"
     ]
    }
   ],
   "source": [
    "SEEDS = [42, 100, 2024]\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 256\n",
    "max_alpha = 1.0\n",
    "\n",
    "# Target location of our saved model\n",
    "models_path = \"/content/drive/MyDrive/DA_Project/models/\"\n",
    "best_model_file_path = os.path.join(models_path, f\"best_cnn_mnist_{BATCH_SIZE}_{NUM_EPOCHS}_{LEARNING_RATE}.pth\")\n",
    "\n",
    "# Store final results\n",
    "final_accuracies = []\n",
    "\n",
    "print(f\"   Starting Robustness Experiment on {len(SEEDS)} seeds...\")\n",
    "print(f\"   Config: BS={BATCH_SIZE}, LR={LEARNING_RATE}, Alpha={max_alpha}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for run_idx, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n\" + \"=\"*40)\n",
    "    print(f\"  RUN {run_idx+1}/{len(SEEDS)} | SEED: {seed}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    # set seed\n",
    "    set_seed(seed)\n",
    "\n",
    "    # Re-Initialize DataLoaders\n",
    "    source_train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    source_test_loader = DataLoader(mnist_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    target_train_loader = DataLoader(mnistm_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "    target_test_loader = DataLoader(mnistm_test, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "\n",
    "    # Initialize DANN\n",
    "    dann_model = DANN_CNN(num_classes=10).to(device)\n",
    "    if os.path.exists(best_model_file_path):\n",
    "        print(f\"Loading pre-trained source weights from {best_model_file_path}...\")\n",
    "        pretrained_state_dict = torch.load(best_model_file_path, map_location=device)\n",
    "\n",
    "        # Create a new state_dict for the DANN model, which we'll populate\n",
    "        dann_state_dict = dann_model.state_dict()\n",
    "\n",
    "        # Copy weights for the feature extractor\n",
    "        for k, v in pretrained_state_dict.items():\n",
    "            if k.startswith('features.'):\n",
    "                dann_state_dict[k] = v\n",
    "            elif k.startswith('classifier.'):\n",
    "                # Rename 'classifier' keys to 'class_classifier' to match DANN_CNN\n",
    "                new_key = k.replace('classifier.', 'class_classifier.')\n",
    "                dann_state_dict[new_key] = v\n",
    "\n",
    "        # Load the modified state_dict. strict=False is used because 'domain_classifier'\n",
    "        # keys will be missing from the loaded pretrained_state_dict, which is intended.\n",
    "        dann_model.load_state_dict(dann_state_dict, strict=False)\n",
    "        print(\"Loaded pre-trained source weights into DANN_CNN (features and class_classifier). Domain classifier initialized randomly.\\n\")\n",
    "    else:\n",
    "        print(\"Pre-trained weights not found. Starting DANN_CNN from scratch.\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    criterion_domain = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(dann_model.parameters(), lr=LEARNING_RATE)\n",
    "    best_target_acc = 0.0\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        dann_model.train()\n",
    "\n",
    "        # Stats tracking\n",
    "        total_loss, total_domain_loss, total_class_loss = 0.0, 0.0, 0.0\n",
    "        total_samples = 0\n",
    "\n",
    "        # Schedule alpha\n",
    "        progress = epoch / NUM_EPOCHS\n",
    "        annealing_factor = 2 / (1 + np.exp(-10 * progress)) - 1\n",
    "        alpha = max_alpha * annealing_factor\n",
    "\n",
    "        # Schedule learning rate\n",
    "        NEW_LEARNING_RATE = LEARNING_RATE / ((1 + 10 * progress) ** 0.75)\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = NEW_LEARNING_RATE\n",
    "\n",
    "        # Zip loaders\n",
    "        min_len = min(len(source_train_loader), len(target_train_loader))\n",
    "        loader_zip = tqdm(zip(source_train_loader, target_train_loader), total=min_len)\n",
    "\n",
    "        for (source_imgs, source_labels), (target_imgs, _) in loader_zip:\n",
    "            # Setup\n",
    "            source_imgs, source_labels = source_imgs.to(device), source_labels.to(device)\n",
    "            target_imgs = target_imgs.to(device)\n",
    "            batch_size = source_imgs.size(0)    # we do this once again to make sure of the batch size\n",
    "\n",
    "            # Handle uneven batches\n",
    "            if source_imgs.size(0) != target_imgs.size(0): continue\n",
    "\n",
    "            # Create Domain Labels: Source = 0, Target = 1\n",
    "            domain_label_s = torch.zeros(batch_size,1).float().to(device)\n",
    "            domain_label_t = torch.ones(batch_size,1).float().to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass (Source Data)\n",
    "            # We calculate BOTH Class Loss and Domain Loss\n",
    "            class_logits_s, domain_logits_s = dann_model(source_imgs, alpha)\n",
    "            loss_class = criterion(class_logits_s, source_labels)\n",
    "            loss_domain_s = criterion_domain(domain_logits_s, domain_label_s)\n",
    "\n",
    "            # Forward Pass (Target Data)\n",
    "            # We ONLY calculate Domain Loss (we don't have class labels)\n",
    "            _, domain_logits_t = dann_model(target_imgs, alpha)\n",
    "            loss_domain_t = criterion_domain(domain_logits_t, domain_label_t)\n",
    "\n",
    "            # Backward Pass (Optimization)\n",
    "            # The GRL inside the model handles the sign flipping automatically!\n",
    "            loss_domain = (loss_domain_s + loss_domain_t)/2\n",
    "            loss = loss_class + loss_domain\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Stats\n",
    "            total_loss += loss.item()\n",
    "            total_class_loss += loss_class.item()\n",
    "            total_domain_loss += loss_domain.item()\n",
    "            total_samples += 1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                prob_s = torch.sigmoid(domain_logits_s).mean().item()\n",
    "                prob_t = torch.sigmoid(domain_logits_t).mean().item()\n",
    "\n",
    "                # Accuracy checks\n",
    "\n",
    "                # Source is correctly classified if logit < 0 (Prob < 0.5)\n",
    "                acc_s = (domain_logits_s < 0).float().mean().item() * 100\n",
    "                # Target is correctly classified if logit > 0 (Prob > 0.5)\n",
    "                acc_t = (domain_logits_t > 0).float().mean().item() * 100\n",
    "\n",
    "                domain_acc = (acc_s + acc_t) / 2\n",
    "\n",
    "            # Logging\n",
    "            loader_zip.set_description(\n",
    "                f\"Ep {epoch+1} | Cls: {loss_class.item():.3f} | \"\n",
    "                f\"Dom Loss: {loss_domain.item():.3f} | \"\n",
    "                f\"Dom Acc: {domain_acc:.1f}%\"\n",
    "            )\n",
    "\n",
    "            # Average classification loss and domain loss\n",
    "            avg_class_loss = total_class_loss / total_samples\n",
    "            avg_domain_loss = total_domain_loss / total_samples\n",
    "\n",
    "        # Validation\n",
    "        # Evaluate on Target (Did adaptation work?)\n",
    "        target_acc = evaluate_accuracy(dann_model, target_test_loader, device)\n",
    "\n",
    "        # Evaluate on Source (Did we preserve original knowledge?)\n",
    "        source_acc = evaluate_accuracy(dann_model, source_test_loader, device)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}] Stats:\")\n",
    "        print(f\"   Avg CL Loss: {avg_class_loss:.4f}\")\n",
    "        print(f\"   Avg Domain Loss: {avg_domain_loss:.4f}\")\n",
    "        print(f\"   Source Acc: {source_acc:.2f}%\")\n",
    "        print(f\"   Target Acc: {target_acc:.2f}%\\n\")\n",
    "\n",
    "        # Checkpointing\n",
    "        if target_acc > best_target_acc:\n",
    "            best_target_acc = target_acc\n",
    "            best_model_wts = copy.deepcopy(dann_model.state_dict())\n",
    "\n",
    "    print(f\"Run {run_idx+1} Complete. Best Acc: {best_target_acc:.2f}%\")\n",
    "    final_accuracies.append(best_target_acc)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "mean_acc = np.mean(final_accuracies)\n",
    "std_acc = np.std(final_accuracies)\n",
    "print(f\"\\n ROBUSTNESS EXPERIMENT COMPLETE WITH ANNEALING\")\n",
    "print(f\"   Individual Runs: {final_accuracies}\")\n",
    "print(f\"   Final Result: {mean_acc:.2f}% ± {std_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB08RWDjaMio"
   },
   "source": [
    "To verify the reliability of the standard annealing strategy, we repeated the experiment across three random seeds (42, 100, and 2024) using the same configuration: source pretraining at Batch 256, followed by adaptation with Alpha Annealing ($0 \\to 1.0$) and Learning Rate Decay.The experiment yielded consistent results with individual accuracies of 84.70%, 80.94%, and 80.66%, resulting in a final mean accuracy of 82.10% ± 1.84%.\n",
    "\n",
    "This result is notably lower than our 'Fixed Alpha' robustness check (89.04% ± 2.01%). The consistency of this performance gap across multiple seeds confirms that the superior performance of the Fixed Alpha strategy was not a fluke. It provides strong empirical evidence that the standard DANN approach of ramping $\\alpha$ to 1.0 is suboptimal for this dataset, likely because the strong adversarial signal eventually interferes with the classification task. The 'Sufficient Alignment' provided by the weaker, fixed adversary consistently preserves more discriminative information."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOpWf2ZPsNUAsjtoBjEuzg",
   "provenance": [
    {
     "file_id": "1ttqwy5R-ERFLizSjRFVdylGP1xB44iKf",
     "timestamp": 1764664992446
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
